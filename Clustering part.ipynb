{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "#pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After web scrapping we created one single data frame with columns 'price', 'locali', 'superficie', 'bagni', 'piano', 'description' and stored it as a .csv file for future usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'locali', 'superficie', 'bagni', 'piano', 'description'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing data into memory from the stored .csv file\n",
    "data = pd.read_csv(\"data_houses.csv\", sep='\\t', encoding='utf-8')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23203, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The number of rows before cleaning\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price             1\n",
       "locali          354\n",
       "superficie       24\n",
       "bagni           491\n",
       "piano          3793\n",
       "description    5045\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking null values of the dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price          object\n",
       "locali         object\n",
       "superficie     object\n",
       "bagni          object\n",
       "piano          object\n",
       "description    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking data types of the dataset\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>locali</th>\n",
       "      <th>superficie</th>\n",
       "      <th>bagni</th>\n",
       "      <th>piano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23202</td>\n",
       "      <td>22849</td>\n",
       "      <td>23179</td>\n",
       "      <td>22712</td>\n",
       "      <td>19410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2335</td>\n",
       "      <td>6</td>\n",
       "      <td>587</td>\n",
       "      <td>315</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>€ 199.000</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>338</td>\n",
       "      <td>7177</td>\n",
       "      <td>962</td>\n",
       "      <td>10007</td>\n",
       "      <td>4236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            price locali superficie  bagni  piano\n",
       "count       23202  22849      23179  22712  19410\n",
       "unique       2335      6        587    315     15\n",
       "top     € 199.000     3          90     1       1\n",
       "freq          338   7177        962  10007   4236"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:,['price','locali','superficie','bagni','piano']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values where the number of bathrooms are 1, 2 or 3 should be kept because they are the most common ones and other numbers \n",
    "#are rare and therefore considered outliers(they are probably buildings_?)\n",
    "#also 3+ is eliminated cause it has ambigious meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      4236\n",
       "2      3308\n",
       "T      2891\n",
       "3      2419\n",
       "A      1652\n",
       "4      1570\n",
       "R      1008\n",
       "5       924\n",
       "6       463\n",
       "S       381\n",
       "7       350\n",
       "8       134\n",
       "9        32\n",
       "11+      26\n",
       "10       16\n",
       "Name: piano, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We decided to drop the rows where the value of the floor is 11+ because there aren't many of those rows and they don't show\n",
    "data.piano.value_counts()\n",
    "#eliminate R, A, S and 11+  cause it is ambigious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After _analyzing_ the possible values of each attribute of the dataset we decided the following about the process of \n",
    "**cleaning** the dataset before making two separate datasets for clustering.\n",
    "    \n",
    "The data cleaning process:\n",
    "    1. Remove nan values\n",
    "    2. Attribute price should be converted to integer data type and '€' removed from the values\n",
    "    3. In attribute locali remove rows where the value of locali is 5+ and convert attribute to integer data type\n",
    "    4. Attribute superficie has some strange values because of the scrapping, like dates '14/02/19' '18/01/19' and all the values that don't make sense should be removed and the attribute should be converted to the integer data type\n",
    "    5. For attribute piano replace T->0 and drop the rows with piano values A, R, S and 11+ cause they are ambiguous\n",
    "    \n",
    "The goal of the cleaning process was to eliminate all the ambiguous values and categorical values that can't have a meaningful numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Method that removes nan values and cleanes the data\n",
    "    \n",
    "    Input: dataframe\n",
    "    Output: cleaned dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #Remove rows where there aren't all values present\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    #try to convert price to int and remove € \n",
    "    for i in data.index:\n",
    "    #Becasuse of the web scrapping some prices had strings and text in this way we dealt with it if exception appears\n",
    "    #'da' is because some prices were scrapped with the word 'da' before the price\n",
    "        try:\n",
    "            data.price[i]=int(data.price[i].replace('€', '').strip().replace('.','').replace('da',''))\n",
    "    #before word class there is price we need\n",
    "        except:\n",
    "            try:\n",
    "                data.price[i]=int(data.price[i].split('class')[0].replace('€', '').strip().replace('.',''))\n",
    "           #in case of the bad scrapping value (e.g.just text) just drop those rows\n",
    "            except:\n",
    "                data.drop(i,inplace=True)\n",
    "            \n",
    "    #convert to int superficie and remove / and . in order to convert to int normally\n",
    "    data.superficie=data.superficie.replace('[/]', '', regex=True).apply(lambda x: int(str(x).replace('.','')))\n",
    "    \n",
    "    #Drop rows where values of locali is 5+ and convert to int\n",
    "    #strip is because of the web scrapping process which took whitespace\n",
    "    data['locali']=data['locali'].apply(lambda x: x.strip())\n",
    "    data.drop(data[ data['locali']=='5+'].index,inplace=True)\n",
    "\n",
    "\n",
    "    #drop A, R, S and 11+  cause it is ambigious \n",
    "    data.drop(data[(data['piano']=='A') | (data['piano']=='R') | ( data['piano']=='S')| ( data['piano']=='11+')].index,inplace=True)\n",
    "    #Attribute piano replace T->0 cause piano T is 'terro' which means it is floor 0\n",
    "    data['piano']=np.where(data['piano']=='T', 0, data['piano'])     \n",
    "    \n",
    "    #remove whitespace from the values\n",
    "    data.bagni=data.bagni.apply(lambda x: x.strip())\n",
    "    #remove all the rows where the value of bagni is not 1,2,3...like 3+, cause other values don't have so many value counts\n",
    "    data.drop(data[( data['bagni']!='1')&( data['bagni']!='2')&( data['bagni']!='3')].index,inplace=True)    \n",
    "\n",
    "    #convert type of \"price\",\"locali\",\"piano\", \"bagni\" to numeric instead of object\n",
    "    data[[\"price\",\"locali\",\"piano\", \"bagni\"]] = data[[\"price\",\"locali\",\"piano\", \"bagni\"]].apply(pd.to_numeric)\n",
    "    \n",
    "    #reset index so it starts from 0 to last row number not with scpapped number indices, but with consistent range\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index',axis=1,inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=10773, step=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>locali</th>\n",
       "      <th>superficie</th>\n",
       "      <th>bagni</th>\n",
       "      <th>piano</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225000</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>papillo eur\\r\\r\\n                                PAPILLO EUR in elegante complesso residenziale rifinitissimo bilocale composto da soggiorno con angolo cottura, stanza da letto bagno e ampio balcone . con Rifiniture di pregio, pavimenti in parquet / grees, infissi in legno con vetro camera e porte in noce, grate nel salone, riscaldamento termoautonomo con caldaia centralizzata, aria condizionata, videocitofono, porta blindata, serramenti elettrici con chiusura centralizzata, antenna satellitare, isolamento termo acustico, pannelli solari e fotovoltaici , rilevatori elettronici di gas. Tutte le camere sono fornite di impianto antifurto, presa antenna satellitare e presa telefonica.div\\r\\r\\ndiv\\r\\r\\nORARI lunedi chiusidiv\\r\\r\\n martedi 10:00-17:00div\\r\\r\\n mercoledi 10:00-17:00div\\r\\r\\ndivgiovedi 10:00-17:00div\\r\\r\\ndivvenrdi 10:00-17:00div\\r\\r\\ndivsabato 10:00-17:00div\\r\\r\\n domenica 10:00-13:00\\r\\r\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  locali  superficie  bagni  piano  \\\n",
       "0  225000  2       50          1      1       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description  \n",
       "0  papillo eur\\r\\r\\n                                PAPILLO EUR in elegante complesso residenziale rifinitissimo bilocale composto da soggiorno con angolo cottura, stanza da letto bagno e ampio balcone . con Rifiniture di pregio, pavimenti in parquet / grees, infissi in legno con vetro camera e porte in noce, grate nel salone, riscaldamento termoautonomo con caldaia centralizzata, aria condizionata, videocitofono, porta blindata, serramenti elettrici con chiusura centralizzata, antenna satellitare, isolamento termo acustico, pannelli solari e fotovoltaici , rilevatori elettronici di gas. Tutte le camere sono fornite di impianto antifurto, presa antenna satellitare e presa telefonica.div\\r\\r\\ndiv\\r\\r\\nORARI lunedi chiusidiv\\r\\r\\n martedi 10:00-17:00div\\r\\r\\n mercoledi 10:00-17:00div\\r\\r\\ndivgiovedi 10:00-17:00div\\r\\r\\ndivvenrdi 10:00-17:00div\\r\\r\\ndivsabato 10:00-17:00div\\r\\r\\n domenica 10:00-13:00\\r\\r\\n              "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10773, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The number of rows after cleaning the dataset\n",
    "#10773\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price          0\n",
       "locali         0\n",
       "superficie     0\n",
       "bagni          0\n",
       "piano          0\n",
       "description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After cleaning the data(dropping rows where there aren't some values) there aren't any NaN values present\n",
    "cleaned_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price           int64\n",
       "locali          int64\n",
       "superficie      int64\n",
       "bagni           int64\n",
       "piano           int64\n",
       "description    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the single data frame we extracted using the web scrapping process we created the Description and the Information datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Description dataset)\n",
    "\n",
    "    columns: description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>papillo eur\\r\\r\\n                                PAPILLO EUR in elegante complesso residenziale rifinitissimo bilocale composto da soggiorno con angolo cottura, stanza da letto bagno e ampio balcone . con Rifiniture di pregio, pavimenti in parquet / grees, infissi in legno con vetro camera e porte in noce, grate nel salone, riscaldamento termoautonomo con caldaia centralizzata, aria condizionata, videocitofono, porta blindata, serramenti elettrici con chiusura centralizzata, antenna satellitare, isolamento termo acustico, pannelli solari e fotovoltaici , rilevatori elettronici di gas. Tutte le camere sono fornite di impianto antifurto, presa antenna satellitare e presa telefonica.div\\r\\r\\ndiv\\r\\r\\nORARI lunedi chiusidiv\\r\\r\\n martedi 10:00-17:00div\\r\\r\\n mercoledi 10:00-17:00div\\r\\r\\ndivgiovedi 10:00-17:00div\\r\\r\\ndivvenrdi 10:00-17:00div\\r\\r\\ndivsabato 10:00-17:00div\\r\\r\\n domenica 10:00-13:00\\r\\r\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description\n",
       "0  papillo eur\\r\\r\\n                                PAPILLO EUR in elegante complesso residenziale rifinitissimo bilocale composto da soggiorno con angolo cottura, stanza da letto bagno e ampio balcone . con Rifiniture di pregio, pavimenti in parquet / grees, infissi in legno con vetro camera e porte in noce, grate nel salone, riscaldamento termoautonomo con caldaia centralizzata, aria condizionata, videocitofono, porta blindata, serramenti elettrici con chiusura centralizzata, antenna satellitare, isolamento termo acustico, pannelli solari e fotovoltaici , rilevatori elettronici di gas. Tutte le camere sono fornite di impianto antifurto, presa antenna satellitare e presa telefonica.div\\r\\r\\ndiv\\r\\r\\nORARI lunedi chiusidiv\\r\\r\\n martedi 10:00-17:00div\\r\\r\\n mercoledi 10:00-17:00div\\r\\r\\ndivgiovedi 10:00-17:00div\\r\\r\\ndivvenrdi 10:00-17:00div\\r\\r\\ndivsabato 10:00-17:00div\\r\\r\\n domenica 10:00-13:00\\r\\r\\n            "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_dataset=pd.DataFrame(cleaned_data.description)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "description_dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Information dataset)\n",
    "\n",
    "    columns: price, locali, superficie, bagni, piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>locali</th>\n",
       "      <th>superficie</th>\n",
       "      <th>bagni</th>\n",
       "      <th>piano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225000</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>339000</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480000</td>\n",
       "      <td>4</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135000</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249000</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  locali  superficie  bagni  piano\n",
       "0  225000  2       50          1      1    \n",
       "1  339000  3       90          1      4    \n",
       "2  480000  4       125         2      4    \n",
       "3  135000  2       60          1      5    \n",
       "4  249000  2       75          1      1    "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_dataset=cleaned_data.drop(['description'],axis=1)\n",
    "information_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this way we can reuse it and not repeat cleaning process every time we execute the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save information and description datasets to two separate .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_dataset.to_csv('information_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_dataset.to_csv('description_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(df):\n",
    "    \"\"\"\n",
    "    Method that returns filtered words from the text input \n",
    "    \n",
    "    Input: string(text)\n",
    "    Output: list(bag of words)\n",
    "    \"\"\"  \n",
    "    #remove upper cases\n",
    "    df=df.lower()\n",
    "   \n",
    "    #replacing new line sign '\\n' '\\r' and 'div' from html with a whitespace ' '    \n",
    "    df=df.replace('\\\\n',' ').replace('\\\\r',' ').replace('div',' ')\n",
    "    \n",
    "    #for removing stop words\n",
    "    stop_words = set(stopwords.words('italian')) \n",
    "    stop_words.add('div')\n",
    "    stop_words.add('n')\n",
    "    stop_words.add('b')\n",
    "\n",
    "    #remove numbers\n",
    "    df = re.sub(\"\\d+\", \"\", df)\n",
    "    \n",
    "    #for removing punctuations\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    #to tokenize the string\n",
    "    word_tokens = tokenizer.tokenize(df)     \n",
    "\n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "    filtered_words = [ps.stem(w) for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(df):\n",
    "    \"\"\"\n",
    "    Method that creates vocabulary\n",
    "    \n",
    "    Input: dataframe\n",
    "    Output: vocabulary list \n",
    "    \"\"\"  \n",
    "    #list for vocabulary \n",
    "    vocabulary_lst=[]\n",
    "    #preprocessing description and get res->list of lists, where each list i a list of filtered preproccesed words \n",
    "    res=df.description.apply(lambda x: preprocessing_text(x))   \n",
    "    \n",
    "    #vocabulary_set is a set used for making vocabulary with unique words\n",
    "\n",
    "    vocabulary_set = set(res[0]).union(*res[1:])\n",
    "\n",
    "    \n",
    "    #mapping words into integers\n",
    "    vocabulary={} \n",
    "    for k,v in enumerate(vocabulary_set):\n",
    "        vocabulary[v]= k\n",
    "    return res,vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res,vocabulary=build_vocabulary(description_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving vocabulary as a dictionary into a \"vocabulary.p\" (pickle) file\n",
    "\n",
    "#21370 what else should we eliminate from the words??\n",
    "pickle.dump(vocabulary, open(\"vocabulary.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load it into memory from file\n",
    "vocabulary = pickle.load(open(\"vocabulary.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "every row is a separate house(which is considered as a separate document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####NEED TO change and finish it in ORDER TO CALCULATE TF_IDF VALUES \n",
    "def compute_inverted_idx(res,vocabulary):\n",
    "    \"\"\"\n",
    "    method that computes an inverted index\n",
    "    \n",
    "    input: res(list of lists), vocabulary(dictionary of all unique words, key=term, value=term_id)\n",
    "    output: inverted_idx(dictionary, key=term_id, value=list of document_ids(rows)) \n",
    "    \"\"\"\n",
    "    #initialize defaultdict for making an inverted index\n",
    "    inverted_idx = defaultdict(list)\n",
    "    #in every document look for every word and assign document id to the words which belong to it\n",
    "    for idx,lst in enumerate(res):\n",
    "        lst=((pd.Series(lst).value_counts())/len(lst)).sort_index()\n",
    "\n",
    "        #lst=set(lst)\n",
    "        for i,tf in enumerate(lst):\n",
    "            inverted_idx[lst.index[i]].append((idx,tf))\n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inverted_idx=compute_inverted_idx(res,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inverted_idx)\n",
    "#dictionary->key=term_id,  value= (document_id,tf_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_idx['saba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(133, 0.009259259259259259),\n",
       " (789, 0.011560693641618497),\n",
       " (1784, 0.020618556701030927),\n",
       " (2166, 0.009433962264150943),\n",
       " (2833, 0.012195121951219513),\n",
       " (3074, 0.006535947712418301),\n",
       " (3098, 0.008130081300813009),\n",
       " (3132, 0.02531645569620253),\n",
       " (3813, 0.012048192771084338),\n",
       " (4098, 0.037037037037037035),\n",
       " (6435, 0.03125),\n",
       " (6635, 0.008130081300813009),\n",
       " (8861, 0.04054054054054054),\n",
       " (8921, 0.017241379310344827),\n",
       " (10079, 0.020618556701030927)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_idx['saba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving inverted_idx as a dictionary into a \"inverted_idx.p\" (pickle) file\n",
    "pickle.dump(inverted_idx, open(\"inverted_idx.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary->key=doc_id,word,  value=tfidf---. sequentially for each word in vocab\n",
    "tf_idf_dic={}\n",
    "total_num_docs=description_dataset.shape[0]\n",
    "\n",
    "    #dictionary->key=term_id, value= (document_id,tf_value)\n",
    "for term,tup_pair in inverted_idx.items():\n",
    "    for doc_id,tf_value in tup_pair:\n",
    "        tf_idf_dic[(doc_id,term)]= np.log(total_num_docs/len(inverted_idx[term]))*tf_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving inverted_idx as a dictionary into a \"inverted_idx.p\" (pickle) file\n",
    "\n",
    "pickle.dump(tf_idf_dic, open(\"tf_idf_dic.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tf_idf_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dic[0,'gree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b8a3118beffa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtemp_row_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdescription_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_row_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2329\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2330\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2331\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2394\u001b[0m         \"\"\"\n\u001b[0;32m   2395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2396\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2397\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2398\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_ensure_valid_index\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   2381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m             self._data = self._data.reindex_axis(value.index.copy(), axis=1,\n\u001b[1;32m-> 2383\u001b[1;33m                                                  fill_value=np.nan)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mreindex_axis\u001b[1;34m(self, new_index, axis, method, limit, fill_value, copy)\u001b[0m\n\u001b[0;32m   3856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3857\u001b[0m         return self.reindex_indexer(new_index, indexer, axis=axis,\n\u001b[1;32m-> 3858\u001b[1;33m                                     fill_value=fill_value, copy=copy)\n\u001b[0m\u001b[0;32m   3859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3860\u001b[0m     def reindex_indexer(self, new_axis, indexer, axis, fill_value=None,\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   3895\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[0;32m   3896\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[1;32m-> 3897\u001b[1;33m                 for blk in self.blocks]\n\u001b[0m\u001b[0;32m   3898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3899\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3895\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[0;32m   3896\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[1;32m-> 3897\u001b[1;33m                 for blk in self.blocks]\n\u001b[0m\u001b[0;32m   3898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3899\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[1;32m-> 1046\u001b[1;33m                                        allow_fill=True, fill_value=fill_value)\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dusica\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[0;32m   1465\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "description_dataset=pd.DataFrame(columns=list(vocabulary.keys()))\n",
    "for row in range(data.shape[0]):\n",
    "    temp_row_values=[]\n",
    "    for word in vocabulary:\n",
    "        try:\n",
    "            temp_row_values.append(tf_idf_dic[(doc_id,word)])\n",
    "        except:\n",
    "            temp_row_values.append(0)\n",
    "    description_dataset[row]=temp_row_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "def calculate_tf_idf(description_dataset,inverted_idx,vocabulary):\n",
    "    \"\"\"\n",
    "    method that calculates tf-idf values\n",
    "     \n",
    "    input:  inverted_idx(dictionary, key=term_id, value=list of document_ids)\n",
    "            vocabulary(dictionary of all unique words, key=term, value=term_id)\n",
    "    output: tf_idf_dic(dictionary of tf_idf_values for all rows(docs), key=tuple(term,doc_id), value=tf_idf value)\n",
    "    \"\"\" \n",
    "    \n",
    "    return tf_idf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load it into memory from file\n",
    "inverted_idx = pickle.load(open(\"inverted_idx.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "This step consists in _clustering the house announcements_ using **K-means++** and choosing the **optimal** number of clusters using the **Elbow-Method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_dataset=pd.read_csv('information_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should we normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information dataset clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=information_dataset.loc[:,['price','locali','superficie','bagni','piano']]\n",
    "distorsions = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k,init='k-means++')\n",
    "    kmeans.fit(X)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(1, 11), distorsions)\n",
    "plt.grid(True)\n",
    "plt.title('Elbow curve')\n",
    "# Inertia: Sum of distances of samples to their closest cluster center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose blabla clusters:\n",
    "Becausee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "kmeans = KMeans(n_clusters=k,init='k-means++')\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroids are:\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_representatives=pd.DataFrame(kmeans.cluster_centers_, columns= information_dataset.columns[1:])\n",
    "cluster_representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=cluster_representatives.melt()\n",
    "b=a.value\n",
    "b1=a.variable\n",
    "plt.plot(b,b1,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_.shape[0]\n",
    "labels=kmeans.labels_\n",
    "centers = np.array(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price centroids\n",
    "centers[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters=kmeans.cluster_centers_.shape[0]\n",
    "\n",
    "#centroidi klastera\n",
    "\n",
    "\n",
    "#Centroid's visualization\n",
    "#for price and superficie\n",
    "centers = np.array(kmeans.cluster_centers_)\n",
    "plt.scatter(centers[:,0], centers[:,2], marker=\"x\", color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description dataset clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
